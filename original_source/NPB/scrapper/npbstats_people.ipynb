{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from tqdm import tqdm\n",
    "\n",
    "def safe_get_and_wait(driver, wait, url, max_retries=2):\n",
    "    \"\"\"Load `url` and wait for table_1 to appear; retry on TimeoutException.\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            wait.until(\n",
    "                EC.presence_of_element_located(\n",
    "                    (By.CSS_SELECTOR, \"#table_1 tbody tr.odd, #table_1 tbody tr.even\")\n",
    "                )\n",
    "            )\n",
    "            time.sleep(0.2)\n",
    "            return True\n",
    "        except TimeoutException:\n",
    "            if attempt < max_retries:\n",
    "                backoff = 2 ** attempt\n",
    "                print(f\"[warn] timeout {url} â€” retry {attempt+1} in {backoff}s\")\n",
    "                time.sleep(backoff)\n",
    "            else:\n",
    "                print(f\"[error] giving up on {url}\")\n",
    "                return False\n",
    "\n",
    "\n",
    "def scrape_npb_bio(ids, chunk_size=5000):\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--window-size=1920,1080\")\n",
    "    opts.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Page.addScriptToEvaluateOnNewDocument\",\n",
    "        {\"source\": \"Object.defineProperty(navigator,'webdriver',{get:()=>undefined})\"},\n",
    "    )\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    out_rows, chunk_idx = [], 1\n",
    "    try:\n",
    "        pbar = tqdm(ids, desc=\"Scraping bio (table_1)\")\n",
    "        for pid in pbar:\n",
    "            pbar.set_postfix(pid=pid)\n",
    "            url = f\"http://npbstats.com/players/db/batting/?wdt_search={pid}\"\n",
    "            if not safe_get_and_wait(driver, wait, url):\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "            # table_1 bio only\n",
    "            bio_tbl = soup.find(\"table\", id=\"table_1\")\n",
    "            if not bio_tbl or bio_tbl.find(\"td\", class_=\"dataTables_empty\"):\n",
    "                continue\n",
    "\n",
    "            bio_df = pd.read_html(str(bio_tbl), flavor=\"lxml\")[0]\n",
    "            bio_df.columns = bio_df.columns.str.strip()\n",
    "            bio = bio_df.iloc[0].to_dict()\n",
    "            bio[\"SearchID\"] = pid\n",
    "            out_rows.append(bio)\n",
    "\n",
    "            if len(out_rows) >= chunk_size:\n",
    "                df_chunk = pd.DataFrame(out_rows)\n",
    "                fname = f\"npbstats_bio_chunk_{chunk_idx}.csv\"\n",
    "                df_chunk.to_csv(fname, index=False)\n",
    "                pbar.write(f\"[saved] {fname}\")\n",
    "                chunk_idx += 1\n",
    "                out_rows.clear()\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ids = [str(pid) for pid in range(100001, 107185)]\n",
    "    df_remain = scrape_npb_bio(ids)\n",
    "    if not df_remain.empty:\n",
    "        df_remain.to_csv(\"npbstats_bio_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
